{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eea5171",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "# HotpotQA: Understanding Multi-hop Question Answering\n",
    "\n",
    "## Introduction\n",
    "\n",
    "HotpotQA represents a significant advancement in question answering research. Created in 2018 by researchers from Carnegie Mellon University, Stanford University, and Université de Montréal, this dataset was specifically designed to address limitations in existing question answering systems and push the boundaries of AI reasoning capabilities.\n",
    "\n",
    "## Why HotpotQA Was Created\n",
    "\n",
    "HotpotQA was developed to address several critical limitations in existing question answering datasets:\n",
    "\n",
    "1. **Limited Reasoning Capabilities**: Prior QA datasets primarily tested single-hop reasoning, where answers could be found in a single paragraph or even a single sentence.\n",
    "\n",
    "2. **Lack of Explainability**: Most QA systems couldn't explain how they arrived at answers, functioning as \"black boxes.\"\n",
    "\n",
    "3. **Constrained by Knowledge Bases**: Many multi-hop datasets were built on structured knowledge bases with predefined schemas, limiting question diversity.\n",
    "\n",
    "4. **Insufficient Supervision**: Previous datasets provided only distant supervision (just the final answer), making it difficult for models to learn reasoning processes.\n",
    "\n",
    "## Key Features of HotpotQA\n",
    "\n",
    "HotpotQA introduced several innovations:\n",
    "\n",
    "1. **Multi-hop Reasoning**: Questions require finding and connecting information across multiple documents, mimicking how humans research complex topics.\n",
    "\n",
    "2. **Natural Language Diversity**: Not constrained by knowledge base schemas, allowing for more natural and diverse questions.\n",
    "\n",
    "3. **Supporting Facts Annotation**: Provides sentence-level supporting facts required for reasoning, enabling stronger supervision and explainable predictions.\n",
    "\n",
    "4. **Comparison Questions**: Introduces a new type of question that requires comparing attributes of different entities.\n",
    "\n",
    "## Dataset Composition\n",
    "\n",
    "- **Size**: Approximately 113,000 question-answer pairs\n",
    "- **Source**: Based on Wikipedia articles\n",
    "- **Question Types**: \n",
    "  - Bridge questions (requiring finding intermediate entities)\n",
    "  - Comparison questions (comparing attributes of entities)\n",
    "- **Difficulty Levels**: Easy, Medium, Hard\n",
    "- **Benchmark Settings**:\n",
    "  - Distractor setting (10 paragraphs with 2 gold paragraphs)\n",
    "  - Full wiki setting (search entire Wikipedia)\n",
    "\n",
    "## Educational Implications\n",
    "\n",
    "HotpotQA is particularly valuable for teaching:\n",
    "\n",
    "1. **Complex Reasoning**: Students can develop step-by-step reasoning skills by understanding how to connect information from multiple sources.\n",
    "\n",
    "2. **Information Synthesis**: The dataset demonstrates how to combine facts from different documents to arrive at new conclusions.\n",
    "\n",
    "3. **Critical Evaluation**: By examining supporting facts, students learn to identify which pieces of information are relevant to answering questions.\n",
    "\n",
    "4. **Explainable AI**: The supporting facts annotation helps teach the importance of explainability in AI systems.\n",
    "\n",
    "## Example Question Types\n",
    "\n",
    "### Bridge Questions\n",
    "\n",
    "These questions require finding an intermediate entity (bridge) that connects information from two documents.\n",
    "\n",
    "**Example**: \n",
    "> \"The Oberoi family is part of a hotel company that has a head office in what city?\"\n",
    "\n",
    "This requires first finding that the Oberoi family is connected to \"The Oberoi Group\" (bridge entity), then discovering where the Oberoi Group headquarters is located.\n",
    "\n",
    "### Comparison Questions\n",
    "\n",
    "These questions require comparing attributes of different entities.\n",
    "\n",
    "**Example**:\n",
    "> \"Were Pavel Urysohn and Leonid Levin both mathematicians?\"\n",
    "\n",
    "This requires finding information about both individuals and comparing their professions.\n",
    "\n",
    "## Challenges and Impact\n",
    "\n",
    "HotpotQA has become a benchmark for evaluating the reasoning capabilities of QA systems. Performance on this dataset measures a system's ability to:\n",
    "\n",
    "1. Find relevant documents\n",
    "2. Identify supporting facts\n",
    "3. Connect information across documents\n",
    "4. Provide accurate answers\n",
    "5. Explain the reasoning process\n",
    "\n",
    "The creation of HotpotQA has inspired numerous advances in multi-hop reasoning models and continues to influence the development of more sophisticated question answering systems.\n",
    "\n",
    "## Resources for Further Learning\n",
    "\n",
    "- [Official HotpotQA Website](https://hotpotqa.github.io/)\n",
    "- [HotpotQA on Hugging Face](https://huggingface.co/datasets/hotpotqa/hotpot_qa)\n",
    "- [GitHub Repository](https://github.com/hotpotqa/hotpot)\n",
    "\n",
    "---\n",
    "\n",
    "*This document is intended for educational purposes to understand the significance of HotpotQA in advancing question answering research and applications.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7532a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv venv && source .venv/bin/activate\n",
    "%pip install -q datasets transformers pytorch-lightning matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fa640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_hotpotqa_dataset():\n",
    "    \"\"\"\n",
    "    Load the HotpotQA dataset from the Hugging Face datasets library.\n",
    "    Returns the dataset object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = load_dataset(\"hotpot_qa\", \"distractor\")\n",
    "        print(\"Successfully loaded HotpotQA dataset!\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Please ensure you have the 'datasets' library installed.\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "hotpotqa = load_hotpotqa_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset_structure(dataset):\n",
    "    \"\"\"\n",
    "    Print basic information about the dataset structure.\n",
    "    \"\"\"\n",
    "    if dataset is None:\n",
    "        print(\"No dataset to explore.\")\n",
    "        return\n",
    "    \n",
    "    # Show available splits\n",
    "    print(f\"Dataset splits: {list(dataset.keys())}\")\n",
    "    \n",
    "    # Show split sizes\n",
    "    for split in dataset.keys():\n",
    "        print(f\"Size of {split} split: {len(dataset[split])}\")\n",
    "    \n",
    "    # Look at column names (features)\n",
    "    print(f\"\\nDataset features: {dataset['train'].features}\")\n",
    "    \n",
    "    # Get a sample from the training set\n",
    "    sample = dataset['train'][0]\n",
    "    \n",
    "    # Print out the structure of the sample\n",
    "    print(\"\\nSample structure:\")\n",
    "    for key in sample:\n",
    "        value = sample[key]\n",
    "        \n",
    "        if isinstance(value, list):\n",
    "            print(f\"{key}: List with {len(value)} items\")\n",
    "            # If the list has elements, show the first one\n",
    "            if len(value) > 0:\n",
    "                if key == \"context\":\n",
    "                    print(f\"  First item: Title = '{value[0][0]}', with {len(value[0][1])} sentences\")\n",
    "                elif key == \"supporting_facts\":\n",
    "                    print(f\"  Format: [document_title, sentence_id]\")\n",
    "                    if len(value) > 0:\n",
    "                        print(f\"  First item: {value[0]}\")\n",
    "                else:\n",
    "                    print(f\"  First item type: {type(value[0])}\")\n",
    "        else:\n",
    "            print(f\"{key}: {type(value)}\")\n",
    "            if isinstance(value, str):\n",
    "                if len(value) > 70:\n",
    "                    print(f\"  Value (truncated): {value[:70]}...\")\n",
    "                else:\n",
    "                    print(f\"  Value: {value}\")\n",
    "    \n",
    "    return sample\n",
    "\n",
    "# Explore the dataset structure\n",
    "sample = explore_dataset_structure(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6947816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset_splits(dataset):\n",
    "    \"\"\"\n",
    "    Analyze how the dataset is divided and what each split contains.\n",
    "    \"\"\"\n",
    "    if dataset is None:\n",
    "        print(\"No dataset to examine.\")\n",
    "        return\n",
    "    \n",
    "    # Get information about each split\n",
    "    split_info = {}\n",
    "    for split in dataset.keys():\n",
    "        split_data = dataset[split]\n",
    "        \n",
    "        # Count question types (based on \"level\" field)\n",
    "        question_types = {}\n",
    "        for i in range(min(1000, len(split_data))):  # Sample the first 1000 examples\n",
    "            example = split_data[i]\n",
    "            q_type = example.get('level', 'Not specified')\n",
    "            if q_type not in question_types:\n",
    "                question_types[q_type] = 0\n",
    "            question_types[q_type] += 1\n",
    "        \n",
    "        # Count yes/no questions\n",
    "        yes_no_count = 0\n",
    "        for i in range(min(1000, len(split_data))):  # Sample the first 1000 examples\n",
    "            example = split_data[i]\n",
    "            if example['answer'].lower() in ['yes', 'no']:\n",
    "                yes_no_count += 1\n",
    "        \n",
    "        # Estimate percentage of yes/no questions\n",
    "        if len(split_data) > 0:\n",
    "            yes_no_percent = yes_no_count / min(1000, len(split_data)) * 100\n",
    "        else:\n",
    "            yes_no_percent = 0\n",
    "        \n",
    "        # Store information about the split\n",
    "        split_info[split] = {\n",
    "            'size': len(split_data),\n",
    "            'question_types': question_types,\n",
    "            'yes_no_question_percent': yes_no_percent,\n",
    "        }\n",
    "    \n",
    "    # Display the information\n",
    "    print(\"Dataset splits analysis:\")\n",
    "    for split, info in split_info.items():\n",
    "        print(f\"\\n{split.upper()} split:\")\n",
    "        print(f\"  Size: {info['size']} examples\")\n",
    "        print(f\"  Yes/No questions: approximately {info['yes_no_question_percent']:.2f}%\")\n",
    "        \n",
    "        if isinstance(info['question_types'], dict) and info['question_types']:\n",
    "            print(\"  Question types:\")\n",
    "            for q_type, count in info['question_types'].items():\n",
    "                percent = count/min(1000, info['size'])*100\n",
    "                print(f\"    {q_type}: {count} sampled examples ({percent:.2f}%)\")\n",
    "    \n",
    "    # Plot the split sizes\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(split_info.keys(), [info['size'] for info in split_info.values()])\n",
    "    plt.title('Dataset Size by Split')\n",
    "    plt.xlabel('Split')\n",
    "    plt.ylabel('Number of Examples')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return split_info\n",
    "\n",
    "# Examine how the dataset is divided\n",
    "split_info = examine_dataset_splits(hotpotqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b95663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_random_examples(dataset, split='train', n=3, max_docs=10, max_sentences=10):\n",
    "    \"\"\"\n",
    "    Display n random examples from the specified split,\n",
    "    handling different data formats in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The HotpotQA dataset\n",
    "        split: Dataset split to sample from ('train', 'validation', etc.)\n",
    "        n: Number of examples to display\n",
    "        max_docs: Maximum number of documents to display per example\n",
    "        max_sentences: Maximum number of sentences to display per document\n",
    "    \n",
    "    Returns:\n",
    "        List of the randomly selected indices\n",
    "    \"\"\"\n",
    "    if dataset is None:\n",
    "        print(\"No dataset to view examples from.\")\n",
    "        return\n",
    "    \n",
    "    if split not in dataset:\n",
    "        print(f\"Split '{split}' not found in dataset.\")\n",
    "        return\n",
    "    \n",
    "    # Get random indices\n",
    "    import random\n",
    "    split_data = dataset[split]\n",
    "    total_examples = len(split_data)\n",
    "    random_indices = random.sample(range(total_examples), min(n, total_examples))\n",
    "    \n",
    "    # Display the random examples\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        example = split_data[idx]\n",
    "        \n",
    "        print(f\"\\n=== Example {i+1} (index {idx}) ===\")\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Answer: {example['answer']}\")\n",
    "        \n",
    "        # Display supporting facts\n",
    "        print(\"\\nSupporting facts:\")\n",
    "        supporting_facts = example['supporting_facts']\n",
    "        \n",
    "        # Handle different formats of supporting facts\n",
    "        if isinstance(supporting_facts, list):\n",
    "            # List format [doc_title, sent_id]\n",
    "            for fact in supporting_facts:\n",
    "                if isinstance(fact, list) and len(fact) >= 2:\n",
    "                    print(f\"  - Document: '{fact[0]}', Sentence ID: {fact[1]}\")\n",
    "                else:\n",
    "                    print(f\"  - {fact}\")\n",
    "        elif isinstance(supporting_facts, dict):\n",
    "            # Dictionary format {'title': [...], 'sent_id': [...]}\n",
    "            titles = supporting_facts.get('title', [])\n",
    "            sent_ids = supporting_facts.get('sent_id', [])\n",
    "            \n",
    "            if len(titles) == len(sent_ids):\n",
    "                for t, s in zip(titles, sent_ids):\n",
    "                    print(f\"  - Document: '{t}', Sentence ID: {s}\")\n",
    "            else:\n",
    "                print(f\"  - Titles: {titles}\")\n",
    "                print(f\"  - Sentence IDs: {sent_ids}\")\n",
    "        else:\n",
    "            print(f\"  - Unknown format: {supporting_facts}\")\n",
    "        \n",
    "        # Display context\n",
    "        print(\"\\nContext:\")\n",
    "        context = example['context']\n",
    "        \n",
    "        # Handle different formats of context\n",
    "        if isinstance(context, list):\n",
    "            # List format of [doc_title, sentences]\n",
    "            display_count = min(len(context), max_docs)\n",
    "            for doc_idx, doc in enumerate(context[:display_count]):\n",
    "                if isinstance(doc, list) and len(doc) >= 2:\n",
    "                    title = doc[0]\n",
    "                    sentences = doc[1]\n",
    "                    print(f\"\\nDocument: '{title}'\")\n",
    "                    \n",
    "                    # Print sentences up to max_sentences\n",
    "                    sentences_display_count = min(len(sentences), max_sentences)\n",
    "                    for sent_idx, sentence in enumerate(sentences[:sentences_display_count]):\n",
    "                        print(f\"  Sentence {sent_idx}: {sentence}\")\n",
    "                    \n",
    "                    if len(sentences) > sentences_display_count:\n",
    "                        print(f\"  ... ({len(sentences) - sentences_display_count} more sentences)\")\n",
    "                else:\n",
    "                    print(f\"\\nDocument {doc_idx+1}: {doc}\")\n",
    "            \n",
    "            if len(context) > display_count:\n",
    "                print(f\"\\n... ({len(context) - display_count} more documents)\")\n",
    "        \n",
    "        elif isinstance(context, dict):\n",
    "            # Dictionary format {'title': [...], 'sentences': [[...]]}\n",
    "            titles = context.get('title', [])\n",
    "            sentences_list = context.get('sentences', [])\n",
    "            \n",
    "            display_count = min(len(titles), max_docs)\n",
    "            for doc_idx, (title, sentences) in enumerate(zip(titles[:display_count], sentences_list[:display_count])):\n",
    "                print(f\"\\nDocument: '{title}'\")\n",
    "                \n",
    "                # Print sentences up to max_sentences\n",
    "                sentences_display_count = min(len(sentences), max_sentences)\n",
    "                for sent_idx, sentence in enumerate(sentences[:sentences_display_count]):\n",
    "                    print(f\"  Sentence {sent_idx}: {sentence}\")\n",
    "                \n",
    "                if len(sentences) > sentences_display_count:\n",
    "                    print(f\"  ... ({len(sentences) - sentences_display_count} more sentences)\")\n",
    "            \n",
    "            if len(titles) > display_count:\n",
    "                print(f\"\\n... ({len(titles) - display_count} more documents)\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"  Unknown format: {context}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    return random_indices\n",
    "\n",
    "# Example usage:\n",
    "# View 2 random examples with up to 10 documents and 10 sentences per document\n",
    "random_indices = view_random_examples(hotpotqa, split='train', n=2, max_docs=10, max_sentences=10)\n",
    "\n",
    "# View 1 random example with more limited output (3 documents, 5 sentences each)\n",
    "# random_indices = view_random_examples(hotpotqa, split='train', n=1, max_docs=3, max_sentences=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
